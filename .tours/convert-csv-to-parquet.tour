{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "Convert CSV to Parquet",
  "steps": [
    {
      "file": "fpl/data/make_parquet.py",
      "description": "**Implement a way to convert our csv to parquet**\n\nWrite code that:\n1. Loads CSV chunk by chunk into memory\n2. Appends the chunk to a parquet dataset.\n3. Has a smart partition to optimize future expected read speeds. \n\nSome tips on the way:\n1. Use [```pandas.read_csv```](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) to load data chunk by chunk (chunk_size)\n2. Parquet files cannot be edited - each chunk will have to be a \"new\" file in a directory. Consult this exellent [guide on parquet datasets](https://arrow.apache.org/docs/python/dataset.html)\n   1. Can each chunk read be written as a new file with a uuid?\n   2. Read the section for partitioning carefully!\n3. You might need to implement a ```read_parquet_to_dataframe``` function. There seems to be a bug with [pandas own](github.com/pandas-dev/pandas/issues/44734).\n   1. Remember to specify partition flavour to \"hive\" on both write and read :-)\n",
      "line": 7,
      "title": "Implement a csv to parquet function here!"
    },
    {
      "file": "fpl/cli.py",
      "description": "**Implement CLI command**\nImplement the CLI command-line interface here!\n\nIn short it should:\n1. Accept a custom csv file - but default to ```data/interim/raw_elements.csv``` \n2. Accept a custom output location - but default to ```data/interim/elements_parquet```\n3. Give a warning and terminate if dataset exist in output location.\n4. Be able to specify columns to partition the dataset on, and a chunk size ",
      "line": 44,
      "title": "Implement CLI interface here!"
    },
    {
      "directory": "tests/cli",
      "description": "**Implement tests**\nAdd your tests here!!!",
      "title": "Implement tests here!"
    }
  ]
}
